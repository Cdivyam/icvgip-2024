<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Discover the dedicated team behind ICVGIP 2024, hosted by IIIT-Bangalore. The page highlights the organizing and steering committees, showcasing the experts steering this leading conference in computer vision, graphics, and image processing. Visit the website for more about the organizers and their roles.">
    <meta name="keywords" content="ICVGIP 2024, About ICVGIP 2024, Submit Paper, ICVGIP 2024 Submission Deadline">
    <meta property="og:title" content="ICVGIP 2024 | IIIT-Bangalore | Plenary Speakers" />
    <meta property="og:description" content="Welcome to 15th Indian Conference on Vision Graphics and Image Processing - ICVGIP 2024 at IIIT-Bangalore" />
    <meta property="og:image" content="https://icvgip.in/assets/iiitb_cover3.jpg" />
    <title>ICVGIP 2024</title>
    <link href="./index-output.css" rel="stylesheet">
    <link rel="canonical" href="https://icvgip.in/plenaryspeakers">
    <link rel="icon" type="image/x-icon" href="assets/favico-v4.png">
</head>

<body class="bg-slate-200" onload="fillOrgPeople()">
    <div class="container max-w-xl lg:max-w-4xl xl:max-w-6xl 2xl:max-w-7xl  mx-auto shadow-2xl">
        <!-- ===========================Header and navbar=========================== -->
        <header class="bg-white">
            <div class="w-full h-96 bg-iiitb-cover-3 bg-cover bg-center align-bottom relative">
                <div class="flex w-100 justify-end">
                    <div class="p-3 w-[90px] sm:w-[110px]">
                        <img src="assets/logos/iuprai-d_1.png" alt="IUPRAI LOGO">
                    </div>
                </div>
                <div class="text-white p-4 absolute bottom-0 ">
                    <div class="font-extrabold font-urbanist text-5xl text-shadow">ICVGIP 2024</div>
                    <div class="font-bold font-urbanist text-2xl text-shadow">IIIT-Bangalore | 13<sup>th</sup> to 15<sup>th</sup> December, 2024</div>
                </div>
            </div>
            <nav id="navbar"
                class="flex flex-col lg:flex-row items-center justify-start bg-navigationPurple-700 text-white text-xl border-t-2 border-white">
                <div id="hamburger-nav-menu" class="w-full flex justify-center lg:hidden p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
                    </svg>
                </div>
                <div id="mobile-nav" class="hidden flex-col w-full lg:hidden items-center justify-center text-center">
                    <a class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer" href="index.html"> Home </a>
                    <div id="subnav-program" class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer">
                        Program
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" class="w-6 h-6 inline-block ml-2">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path>
                        </svg>
                    </div>
                    <a class="subnav-program hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="program.html"> Conference Program </a>
                    <a class="subnav-program hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="tutorials.html"> Tutorials </a>
                    <a class="subnav-program hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="plenaryspeakers.html"> Plenary Speakers </a>
                    <a class="subnav-program hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="accepted-regular-papers.html"> Accepted Regular Papers </a>
                    <a class="subnav-program hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="accepted-symposium-papers.html"> Accepted Symposium Papers </a>
                    <a class="subnav-program hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="accepted-tiny-papers.html"> Accepted Tiny Papers </a>
                    <div id="subnav-infoatt" class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer">
                        Info For Attendees
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" class="w-6 h-6 inline-block ml-2">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path>
                        </svg>
                    </div>
                    <a class="subnav-infoatt hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="registration.html"> Registration </a>
                    <a class="subnav-infoatt hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="accommodation.html"> Accommodation </a>
                    <a class="subnav-infoatt hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="venue.html"> Venue </a>
                    <div id="subnav-infoauth" class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer">
                        Info For Authors
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"
                            class="w-6 h-6 inline-block ml-2">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path>
                        </svg>
                    </div>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="iuprai-doctoral-dissertation-award.html"> IUPRAI Doctoral Dissertation Award </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="vision-india.html"> Vision INDIA </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="call-for-tutorials.html"> Call For Tutorials </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="call-for-tiny-papers.html"> Call For Tiny Papers </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="young-researcher-symposium.html">  Young Researchers' Symposium </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="young-faculty-symposium.html">  Young Faculty' Symposium </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="industry-research-symposium.html">  Industry Research Symposium </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="dates.html"> Important Dates </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="cfp.html"> Call For papers </a>
                    <a class="subnav-infoauth hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="regular-paper-submission.html"> Regular Paper Submission </a>
                    <a class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer" href="plenaryspeakers.html"> Plenary Speakers </a>
                    <div id="subnav-committees" class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer">
                        Committees
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" class="w-6 h-6 inline-block ml-2">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path>
                        </svg>
                    </div>
                    <a class="subnav-committees hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="organisers.html"> Organising Committee </a>
                    <a class="subnav-committees hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="steering.html"> Steering Committee </a>
                    <a class="subnav-committees hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="plenarychair.html"> Plenary Chair </a>
                    
                    <a class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer" href="sponsorship.html"> Sponsorship </a>  
                    <div id="subnav-archive" class="w-full p-2 hover:bg-navigationPurple-500 hover:cursor-pointer">
                        Archive
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" class="w-6 h-6 inline-block ml-2">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"></path>
                        </svg>
                    </div>
                    <a class="subnav-archive-year hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="https://www.iitrpr.ac.in/ICVGIP/" target="_blank"> 2023 </a>
                    <a class="subnav-archive-year hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="https://events.iitgn.ac.in/2022/icvgip/index.html" target="_blank"> 2022 </a>              
                    <a class="subnav-archive-year hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="https://iitj.ac.in/icvgip2021/index.php" target="_blank"> 2021 </a>              
                    <a class="subnav-archive-year hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="https://iitj.ac.in/icvgip2021/2020/index.php" target="_blank"> 2020 </a>              
                    <a class="subnav-archive-year hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="https://cvit.iiit.ac.in/icvgip18/" target="_blank"> 2018 </a>               
                    <a class="subnav-archive-year hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="https://dl.acm.org/doi/proceedings/10.1145/3009977" target="_blank"> 2016 </a>              
                    <a class="subnav-archive-year hidden w-full p-2 bg-navigationPurple-500 hover:bg-navigationPurple-700 hover:cursor-pointer" href="https://dl.acm.org/doi/proceedings/10.1145/2683483" target="_blank"> 2014 </a>                            </div>
                <a class="hidden lg:block p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer " href="index.html">Home</a>
                <div class="hidden lg:block relative group p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer border-l-2">
                    Program
                    <div class="absolute hidden group-hover:block bg-navigationPurple-700 shadow-md mt-4 w-[220%] z-10 left-0">
                        <a href="program.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Conference Program</a>
                        <a href="tutorials.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Tutorials</a>
                        <a href="plenaryspeakers.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Plenary Speakers</a>
                        <a href="accepted-regular-papers.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Accepted Regular Papers</a>
                        <a href="accepted-symposium-papers.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Accepted Symposium Papers</a>                        
                        <a href="accepted-tiny-papers.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Accepted Tiny Papers</a>
                    </div>
                </div>
                <div class="hidden lg:block relative group p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer border-l-2">
                    Info For Attendees
                    <div class="absolute hidden group-hover:block bg-navigationPurple-700 shadow-md mt-4 w-[120%] z-10 left-0">
                        <a href="registration.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Registration</a>
                        <a href="accommodation.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Accommodation</a>
                        <a href="venue.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Venue</a>
                    </div>
                </div>
                <div class="hidden lg:block relative group p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer border-l-2">
                    Info For Authors
                    <div
                        class="absolute hidden group-hover:block bg-navigationPurple-700 shadow-md mt-4 w-[162%] z-10 left-0">
                        <a href="iuprai-doctoral-dissertation-award.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">IUPRAI Doctoral Dissertation Award</a>
                        <a href="vision-india.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Vision INDIA</a>
                        <a href="call-for-tutorials.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Call For Tutorials</a>
                        <a href="call-for-tiny-papers.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Call For Tiny Papers</a>
                        <a href="young-researcher-symposium.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Young Researchers' Symposium</a>
                        <a href="young-faculty-symposium.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Young Faculty' Symposium</a>
                        <a href="industry-research-symposium.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Industry Research Symposium</a>
                        <a href="dates.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Important Dates</a>
                        <a href="cfp.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Call For papers</a>
                        <a href="regular-paper-submission.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Regular Paper Submission</a>
                    </div>
                </div>
                <a class="hidden lg:block p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer border-l-2" href="plenaryspeakers.html">Plenary Speakers</a>
                <div class="hidden lg:block relative group p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer border-l-2">
                    Committees
                    <div class="absolute hidden group-hover:block bg-navigationPurple-700 shadow-md mt-4 w-[155%] z-10 left-0">
                        <a href="organisers.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Organising Committee</a>
                        <a href="steering.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Steering Committee</a>
                        <a href="plenarychair.html" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">Plenary Chair</a>
                    </div>
                </div>
                
                
                <a class="hidden lg:block p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer border-l-2" href="sponsorship.html">Sponsorship</a>
                <div class="hidden lg:block relative group p-4 lg:px-6 hover:bg-navigationPurple-500 hover:cursor-pointer border-x-2">
                    Archive
                    <div class="absolute hidden group-hover:block bg-navigationPurple-700 shadow-md mt-4 w-full z-10 left-0">
                        <a href="https://www.iitrpr.ac.in/ICVGIP/" target="_blank" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">2023</a>
                        <a href="https://events.iitgn.ac.in/2022/icvgip/index.html" target="_blank" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">2022</a>
                        <a href="https://iitj.ac.in/icvgip2021/index.php" target="_blank" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">2021</a>
                        <a href="https://iitj.ac.in/icvgip2021/2020/index.php" target="_blank" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">2020</a>
                        <a href="https://cvit.iiit.ac.in/icvgip18/" target="_blank" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">2018</a>
                        <a href="https://dl.acm.org/doi/proceedings/10.1145/3009977" target="_blank" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">2016</a>
                        <a href="https://dl.acm.org/doi/proceedings/10.1145/2683483" target="_blank" class="block p-4 text-white hover:bg-navigationPurple-500 border-y-white border-t-2">2014</a>
                    </div>
                </div> 
                <!-- <div
                    class="hidden lg:flex items-center justify-evenly p-4 lg:px-6 gap-x-2 hover:bg-navigationPurple-500 hover:cursor-pointer
                            border-x-2">
                    Text
                    <svg class="h-5 w-5 flex-none text-gray-400" viewBox="0 0 20 20" fill="currentColor"
                        aria-hidden="true">
                        <path fill-rule="evenodd"
                            d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
                            clip-rule="evenodd" />
                    </svg>
                </div> -->
                <!-- <a class="lg:block p-4 lg:px-6 hover:bg-navigationPurple-400 hover:cursor-pointer text-navigationPurple-700">
                </a> -->
                <!-- <div class="p-6 lg:px-6">Test</div>
                 -->
            </nav>            
        </header>

        <!-- ====================================================== -->
        <!-- Content -->
        <div class="flex flex-col bg-white lg:px-16">
            
            <div class="text-4xl p-6">Plenary Speakers</div>
            <hr class="h-px mx-5 bg-slate-400 border-0">
            <div class="speaker">
                <div class="flex flex-col lg:flex-row items-center lg:items-start w-full p-2 lg:p-5">
                    <div class="flex w-[80%] sm:w-1/2 md:w-[250px] justify-center">
                        <img src="assets/AlBovik.jpg" alt="Alan Bovik">
                    </div>
                    <div class="w-full flex flex-col p-2 lg:px-5 lg:py-0">
                        <div class="font-urbanist text-xl font-bold text-center lg:text-left text-navigationPurple-700">On Visual Quality: Pictures, Videos, and GenAI</div>
                        <div class="text-md font-bold text-center lg:text-left">Alan C. Bovik</div>
                        <div class="italic text-center lg:text-left">Professor, The University of Texas at Austin</div>   
                        <div class="h-[0.5px] w-full bg-slate-400 my-1"></div>
                        <div class="text-sm text-justify px-3 lg:px-0 pb-6">
                            <p><strong>Abstract: </strong> Predicting the perceptual quality of pictures and videos 
                                is a hard problem that has been successfully addressed in many scenarios, such 
                                as quality control of streaming videos and sharing of social media pictures. 
                                In this talk I will address how visual quality perception can be understood 
                                using principles of neuroscience and neuro-statistical models of distortion. 
                                In particular I will review some basic vision science that makes accurate 
                                perceptual visual quality prediction possible, and how algorithms are designed 
                                that are now used worldwide. However, GenAI pictures are fundamentally different 
                                from optical pictures, and those differences may be statistically testable. 
                                I will an early exploration into the perceptual statistics of GenAI pictures, 
                                and how picture quality models can be used to gauge GenAI visual quality and 
                                detectability.
                            </p>
                            <p><strong>Bio:</strong> Al Bovik is the Cockrell Family Regents Endowed Chair 
                                Professor at The University of Texas at Austin. His research interests land 
                                at the nexus of visual neuroscience and digital pictures and videos, and 
                                how their capture, processing, transmission, and display can be perceptually 
                                optimized. An elected member of the U.S. National Academy of Engineering, the 
                                Indian National Academy of Engineering, the National Academy of Inventors, and 
                                Academia Europaea, his many honors include the John Fritz Medal, the IEEE 
                                Edison Medal, a Primetime Emmy Award, a Technology & Engineering Emmy Award, 
                                the RPS Progress Medal, the IEEE Fourier Award, and the Edwin H. Land Medal.
                            </p>
                        </div>                 
                    </div>
                </div>
                <div class="text-sm px-6 text-justify pb-6 hidden">
                    <p><strong>Abstract: </strong> Predicting the perceptual quality of pictures and videos 
                        is a hard problem that has been successfully addressed in many scenarios, such 
                        as quality control of streaming videos and sharing of social media pictures. 
                        In this talk I will address how visual quality perception can be understood 
                        using principles of neuroscience and neuro-statistical models of distortion. 
                        In particular I will review some basic vision science that makes accurate 
                        perceptual visual quality prediction possible, and how algorithms are designed 
                        that are now used worldwide. However, GenAI pictures are fundamentally different 
                        from optical pictures, and those differences may be statistically testable. 
                        I will an early exploration into the perceptual statistics of GenAI pictures, 
                        and how picture quality models can be used to gauge GenAI visual quality and 
                        detectability.
                    </p>
                    <p><strong>Bio:</strong> Al Bovik is the Cockrell Family Regents Endowed Chair 
                        Professor at The University of Texas at Austin. His research interests land 
                        at the nexus of visual neuroscience and digital pictures and videos, and 
                        how their capture, processing, transmission, and display can be perceptually 
                        optimized. An elected member of the U.S. National Academy of Engineering, the 
                        Indian National Academy of Engineering, the National Academy of Inventors, and 
                        Academia Europaea, his many honors include the John Fritz Medal, the IEEE 
                        Edison Medal, a Primetime Emmy Award, a Technology & Engineering Emmy Award, 
                        the RPS Progress Medal, the IEEE Fourier Award, and the Edwin H. Land Medal.
                    </p>
                </div>
            </div>
            <div class="speaker">
                <div class="flex flex-col lg:flex-row items-center lg:items-start w-full p-2 lg:p-5">
                    <div class="flex w-[80%] sm:w-1/2 md:w-[250px] justify-center">
                        <img src="assets/ArshaN_v2.jpg" alt="Arsha Nagrani">
                    </div>
                    <div class="w-full flex flex-col p-2 lg:px-5 lg:py-0">
                        <div class="font-urbanist text-xl font-bold text-center lg:text-left text-navigationPurple-700">Long Video Understanding in the age of large MLMs</div>
                        <div class="text-md font-bold text-center lg:text-left">Arsha Nagrani</div>
                        <div class="italic text-center lg:text-left">Staff Research Scientist, Google Deepmind</div>   
                        <div class="h-[0.5px] w-full bg-slate-400 my-1"></div>
                        <div class="text-sm text-justify px-3 lg:px-0 pb-6">
                            <p><strong>Abstract:</strong> What makes understanding videos so challenging for large 
                                multimodal language models, such as Gemini and GPT4? We will dive into some of the 
                                challenges, including fun new tasks, datasets, evaluations and models, covering 
                                recently accepted papers at CVPR and NeurIPS 2024. 
                            </p>
                            <p><strong>Bio:</strong> Arsha Nagrani is a Staff Research Scientist at Google DeepMind. 
                                She obtained her PhD from the VGG group in the University of Oxford with Andrew Zisserman, 
                                where her thesis received the ELLIS PhD Award. Prior to that, she received her BA and MEng degrees from the 
                                University of Cambridge, UK. Her work has been recognised by a Best Student Paper Award at Interspeech, 
                                an Outstanding Paper Award at ICASSP, a Google PhD Fellowship and a Townsend Scholarship, 
                                and has been covered by news outlets such as The New Scientist, MIT Tech review and Verdict. 
                                Her research is focused on machine learning techniques for video understanding.
                            </p>
                        </div>                 
                    </div>
                </div>
                <div class="text-sm px-6 text-justify pb-6 hidden">
                    <p><strong>Abstract:</strong> What makes understanding videos so challenging for large 
                        multimodal language models, such as Gemini and GPT4? We will dive into some of the 
                        challenges, including fun new tasks, datasets, evaluations and models, covering 
                        recently accepted papers at CVPR and NeurIPS 2024. 
                    </p>
                    <p><strong>Bio:</strong> Arsha Nagrani is a Staff Research Scientist at Google DeepMind. 
                        She obtained her PhD from the VGG group in the University of Oxford with Andrew Zisserman, 
                        where her thesis received the ELLIS PhD Award. Prior to that, she received her BA and MEng degrees from the 
                        University of Cambridge, UK. Her work has been recognised by a Best Student Paper Award at Interspeech, 
                        an Outstanding Paper Award at ICASSP, a Google PhD Fellowship and a Townsend Scholarship, 
                        and has been covered by news outlets such as The New Scientist, MIT Tech review and Verdict. 
                        Her research is focused on machine learning techniques for video understanding.
                    </p>
                </div>
            </div>
            <div class="speaker">
                <div class="flex flex-col lg:flex-row items-center lg:items-start w-full p-2 lg:p-5">
                    <div class="flex w-[80%] sm:w-1/2 md:w-[250px] justify-center">
                        <img src="assets/devaR_v1.jpg" alt="Deva Ramanan">
                    </div>
                    <div class="w-full flex flex-col p-2 lg:px-5 lg:py-0">
                        <div class="font-urbanist text-xl font-bold text-center lg:text-left text-navigationPurple-700">Multimodal Spatial Intelligence for Interacting in a Dynamic World</div>
                        <div class="text-md font-bold text-center lg:text-left">Deva Ramanan</div>
                        <div class="italic text-center lg:text-left">Professor, Robotics Institute, Carnegie Mellon University</div>   
                        <div class="h-[0.5px] w-full bg-slate-400 my-1"></div>
                        <div class="text-sm text-justify px-3 lg:px-0 pb-6">
                            <p><strong>Abstract:</strong> Artificial intelligence and machine learning are enjoying a 
                                period of tremendous progress, driven in large part by scale, compute, and learnable 
                                neural representations. However, such innovations have yet to translate to the physical 
                                world, as technologies such as self-driving vehicles are still restricted to limited 
                                deployments. In this talk, I will argue that autonomy requires spatial three-dimensional 
                                understanding integrated with intuitive physical models of a changing world. To do so, 
                                I will discuss a variety of models that revisit classic "analysis by synthesis" 
                                approaches to scene understanding, taking advantage of recent advances in differentiable 
                                rendering and simulation. But to enable data-driven autonomy for safety-critical 
                                applications, I will also argue that the community needs new perspectives on data 
                                curation and annotation. Toward this end, I will discuss approaches that leverage 
                                multimodal vision-language models to better characterize datasets and models.
                            </p>
                            <p><strong>Bio:</strong> Deva Ramanan is a Professor in the Robotics Institute at Carnegie-
                                Mellon University and the former director of the CMU Center for Autonomous Vehicle Research.
                                His research interests span computer vision and machine learning, with
                                a focus on visual recognition. He was awarded the David Marr Prize in 2009, the
                                PASCAL VOC Lifetime Achievement Prize in 2010, the IEEE PAMI Young Researcher Award
                                in 2012, named one of Popular Science's Brilliant 10 researchers in 2012, named a
                                National Academy of Sciences Kavli Fellow in 2013, won the Longuet-Higgins Prize for
                                fundamental contributions in computer vision in both 2018 and 2024, and was recognized
                                for best paper finalist / honorable mention awards in CVPR 2019, ECCV 2020, and ICCV
                                2021. His work is supported by NSF, ONR, DARPA, as well as industrial collaborations
                                with Intel, Google, and Microsoft. He served at the program chair of the IEEE Computer Vision and Pattern Recognition
                                (CVPR) 2018. He is on the editorial board of the International Journal of Computer
                                Vision (IJCV) and is an associate editor for the IEEE Transactions on Pattern Analysis
                                and Machine Intelligence (PAMI). He regularly serves as a senior program committee
                                member for CVPR, the International Conference on Computer Vision (ICCV), and the
                                European Conference on Computer Vision (ECCV). He also regularly serves on NSF panels
                                for computer vision and machine learning.
                            </p>
                        </div>                 
                    </div>
                </div>
                <div class="text-sm px-6 text-justify pb-6 hidden">
                    <p><strong>Abstract:</strong> Artificial intelligence and machine learning are enjoying a 
                        period of tremendous progress, driven in large part by scale, compute, and learnable 
                        neural representations. However, such innovations have yet to translate to the physical 
                        world, as technologies such as self-driving vehicles are still restricted to limited 
                        deployments. In this talk, I will argue that autonomy requires spatial three-dimensional 
                        understanding integrated with intuitive physical models of a changing world. To do so, 
                        I will discuss a variety of models that revisit classic "analysis by synthesis" 
                        approaches to scene understanding, taking advantage of recent advances in differentiable 
                        rendering and simulation. But to enable data-driven autonomy for safety-critical 
                        applications, I will also argue that the community needs new perspectives on data 
                        curation and annotation. Toward this end, I will discuss approaches that leverage 
                        multimodal vision-language models to better characterize datasets and models.
                    </p>
                    <p><strong>Bio:</strong> Deva Ramanan is a Professor in the Robotics Institute at Carnegie-
                        Mellon University and the former director of the CMU Center for Autonomous Vehicle Research.
                        His research interests span computer vision and machine learning, with
                        a focus on visual recognition. He was awarded the David Marr Prize in 2009, the
                        PASCAL VOC Lifetime Achievement Prize in 2010, the IEEE PAMI Young Researcher Award
                        in 2012, named one of Popular Science's Brilliant 10 researchers in 2012, named a
                        National Academy of Sciences Kavli Fellow in 2013, won the Longuet-Higgins Prize for
                        fundamental contributions in computer vision in both 2018 and 2024, and was recognized
                        for best paper finalist / honorable mention awards in CVPR 2019, ECCV 2020, and ICCV
                        2021. His work is supported by NSF, ONR, DARPA, as well as industrial collaborations
                        with Intel, Google, and Microsoft. He served at the program chair of the IEEE Computer Vision and Pattern Recognition
                        (CVPR) 2018. He is on the editorial board of the International Journal of Computer
                        Vision (IJCV) and is an associate editor for the IEEE Transactions on Pattern Analysis
                        and Machine Intelligence (PAMI). He regularly serves as a senior program committee
                        member for CVPR, the International Conference on Computer Vision (ICCV), and the
                        European Conference on Computer Vision (ECCV). He also regularly serves on NSF panels
                        for computer vision and machine learning.
                    </p>
                </div>
            </div>
            <div class="speaker">
                <div class="flex flex-col lg:flex-row items-center lg:items-start w-full p-2 lg:p-5">
                    <div class="flex w-[80%] sm:w-1/2 md:w-[250px] justify-center">
                        <img src="assets/SPArun_v2.jpg" alt="SP Arun">
                    </div>
                    <div class="w-full flex flex-col p-2 lg:px-5 lg:py-0">
                        <div class="font-urbanist text-xl font-bold text-center lg:text-left text-navigationPurple-700">Towards a real conversation between vision in brains and machines</div>
                        <div class="text-md font-bold text-center lg:text-left">SP Arun</div>
                        <div class="italic text-center lg:text-left">Professor, Centre for Neuroscience, IISc Bangalore</div>   
                        <div class="h-[0.5px] w-full bg-slate-400 my-1"></div>
                        <div class="text-sm text-justify px-3 lg:px-0 pb-6">
                            <p><strong>Bio:</strong> SP Arun started out as an electrical engineer, read too much science 
                                fiction for his own good and turned into a neuroscientist. He is fascinated by how the brain 
                                transforms sensation into perception, particularly for vision. His lab at the Centre for 
                                Neuroscience, Indian Institute of Science, studies how the brain solves vision by investigating 
                                perception and brain activity in humans, by investigating behavior and neural activity in 
                                monkeys and by comparing vision in brains and machine algorithms. For more details visit the 
                                homepage of his research group, the 
                                <a href="https://sites.google.com/site/visionlabiisc/" target="_blank" class="text-blue-500 underline">Vision Lab @ IISc.</a>
                            </p>
                            
                        </div>                 
                    </div>
                </div>
                <div class="text-sm px-6 text-justify pb-6 hidden">
                    <p><strong>Bio:</strong> SP Arun started out as an electrical engineer, read too much science 
                        fiction for his own good and turned into a neuroscientist. He is fascinated by how the brain 
                        transforms sensation into perception, particularly for vision. His lab at the Centre for 
                        Neuroscience, Indian Institute of Science, studies how the brain solves vision by investigating 
                        perception and brain activity in humans, by investigating behavior and neural activity in 
                        monkeys and by comparing vision in brains and machine algorithms. For more details visit the 
                        homepage of his research group, the 
                        <a href="https://sites.google.com/site/visionlabiisc/" target="_blank" class="text-blue-500 underline">Vision Lab @ IISc.</a>
                    </p>
                </div>
            </div>
            <div class="speaker">
                <div class="flex flex-col lg:flex-row items-center lg:items-start w-full p-2 lg:p-5">
                    <div class="flex flex-col w-[80%] sm:w-1/2 md:w-[250px] justify-center">
                        <img src="assets/VN.jpg" alt="Vasilis Ntziachristos">
                        <span style="font-size:53%">Photo © Stephan Rumpf/Helmholtz Zentrum München</span>
                    </div>
                    <div class="w-full flex flex-col p-2 lg:px-5 lg:py-0">
                        <div class="font-urbanist text-xl font-bold text-center lg:text-left text-navigationPurple-700">Looking and Listening to Light: Advances in Optical and Optoacoustic Imaging</div>
                        <div class="text-md font-bold text-center lg:text-left">Vasilis Ntziachristos</div>
                        <div class="italic text-center lg:text-left">Professor and Chair of Biological Imaging (CBI) - Technical University Munich, Germany</div>   
                        <div class="italic text-center lg:text-left">Director, Institute of Biological and Medical Imaging (IBMI) – Helmholtz Munich, Germany</div>   
                        <div class="h-[0.5px] w-full bg-slate-400 my-1"></div>
                        <div class="text-sm text-justify px-3 lg:px-0 pb-6">
                            <p><strong>Bio:</strong> Professor Vasilis Ntziachristos studied electrical 
                                engineering at Aristotle University in Thessaloniki. Following his M.Sc. and 
                                Ph.D. in the Department of Bioengineering at the University of Pennsylvania, he was then 
                                appointed Assistant Professor and Director of the Laboratory for Bio-Optics and Molecular 
                                Imaging at Harvard University and Massachusetts General Hospital. Since 2007, he 
                                has served as Professor of Medicine and Electrical Engineering and the Chair of 
                                Biological Imaging at the Technical University of Munich and Director of the 
                                Institute of Biological and Medical Imaging at Helmholtz Munich. Prof. 
                                Ntziachristos is also currently Director of Bioengineering at the 
                                Helmholtz Pioneer Campus, the Head of the Bioengineering Department at 
                                Helmholtz Munich, and Director of the IESL at FORTH. Prof. Ntziachristos is 
                                the founder of the journal Photoacoustics, regularly Chairs in international 
                                meetings and councils and has received numerous awards and distinctions, 
                                including the Karl Heinz Beckurts prize (2021), the Chaire Blaise Pascal 
                                (2019) from the Region Ile-de-France, the Gold Medal from the Society for 
                                Molecular Imaging (2015), the Gottfried Leibnitz prize from the German 
                                Research Foundation (2013), the Erwin Schrödinger Award (2012) and was named 
                                one of the world's top innovators by the Massachusetts Institute of Technology 
                                (MIT) Technology Review in 2004. In 2024, he has been elected as a new member 
                                of the German Academy of Sciences Leopoldina.
                            </p>
                            
                        </div>                 
                    </div>
                </div>
                <div class="text-sm px-6 text-justify pb-6 hidden">
                    <p><strong>Bio:</strong> Professor Vasilis Ntziachristos studied electrical 
                        engineering at Aristotle University in Thessaloniki. Following his M.Sc. and 
                        Ph.D. in the Department of Bioengineering at the University of Pennsylvania, he was then 
                        appointed Assistant Professor and Director of the Laboratory for Bio-Optics and Molecular 
                        Imaging at Harvard University and Massachusetts General Hospital. Since 2007, he 
                        has served as Professor of Medicine and Electrical Engineering and the Chair of 
                        Biological Imaging at the Technical University of Munich and Director of the 
                        Institute of Biological and Medical Imaging at Helmholtz Munich. Prof. 
                        Ntziachristos is also currently Director of Bioengineering at the 
                        Helmholtz Pioneer Campus, the Head of the Bioengineering Department at 
                        Helmholtz Munich, and Director of the IESL at FORTH. Prof. Ntziachristos is 
                        the founder of the journal Photoacoustics, regularly Chairs in international 
                        meetings and councils and has received numerous awards and distinctions, 
                        including the Karl Heinz Beckurts prize (2021), the Chaire Blaise Pascal 
                        (2019) from the Region Ile-de-France, the Gold Medal from the Society for 
                        Molecular Imaging (2015), the Gottfried Leibnitz prize from the German 
                        Research Foundation (2013), the Erwin Schrödinger Award (2012) and was named 
                        one of the world's top innovators by the Massachusetts Institute of Technology 
                        (MIT) Technology Review in 2004. In 2024, he has been elected as a new member 
                        of the German Academy of Sciences Leopoldina.
                    </p>
                </div>
            </div>
            <div class="speaker">
                <div class="flex flex-col lg:flex-row items-center lg:items-start w-full p-2 lg:p-5">
                    <div class="flex flex-col w-[80%] sm:w-1/2 md:w-[250px] justify-center">
                        <img src="assets/GD2013.jpg" alt="George Drettakis">
                        <span style="font-size:53%">Photo © 2013 Anna Drettakis</span>
                    </div>
                    <div class="w-full flex flex-col p-2 lg:px-5 lg:py-0">
                        <div class="font-urbanist text-xl font-bold text-center lg:text-left text-navigationPurple-700">The 3D Gaussian Splatting Adventure: Past, Present and Future</div>
                        <div class="text-md font-bold text-center lg:text-left">George Drettakis</div>
                        <div class="italic text-center lg:text-left">Senior Researcher, INRIA</div>   
                        <div class="h-[0.5px] w-full bg-slate-400 my-1"></div>
                        <div class="text-sm text-justify px-3 lg:px-0 pb-6">
                            <p><strong>Abstract:</strong> Neural rendering has advanced at outstanding speed in 
                                recent years, with the advent of Neural Radiance Fields (NeRFs), 
                                typically based on volumetric ray-marching. Last year, our group developed an 
                                alternative approach, 3D Gaussian Splatting, that has better performance for 
                                training, display speed and visual quality and has seen widespread adoption both 
                                academically and industrially. In this talk, we describe the 20+ year process 
                                leading to the development of this method and discuss some future directions. 
                                We will start with a short historical perspective of our work on image-based and 
                                neural rendering over the years, outlining several developments that guided our 
                                thinking over the years. We then discuss a sequence of three point-based 
                                rasterization methods for novel view synthesis -- developed in the context of G. 
                                Kopanas' Ph.D. and the ERC Advanced Grant FUNGRAPH -- that culminated with 3D 
                                Gaussian Splatting. We will emphasize how we progressively overcame the challenges as 
                                the research progressed. We first discuss differentiable point splatting and how 
                                we extended it in our first approach that enhances points with neural features, 
                                optimizing geometry to correct reconstruction errors. We briefly review our 
                                second method that handles highly reflective objects, where we use multi-layer 
                                perceptrons (MLP), to learn the motion of reflections and to perform the final 
                                rendering of captured scenes. We then discuss 3D Gaussian Splatting, that 
                                provides the high-quality real-time rendering for novel view synthesis using a 
                                novel 3D scene representation based on 3D Gaussians and fast GPU rasterization. 
                                We will conclude with a discussion of future directions for 3D Gaussian 
                                splatting with examples from recent work.
                            </p>
                            <p><strong>Bio:</strong> George Drettakis graduated in Computer Science from the 
                                University of Crete, Greece, and obtained an M.Sc. and a Ph.D., (1994) at the 
                                University of Toronto, with E. Fiume. After an ERCIM postdoc in Grenoble, 
                                Barcelona and Bonn, he obtained a Inria researcher position in Grenoble in 1995, 
                                and his "Habilitation" at the University of Grenoble (1999). He then founded the 
                                REVES research group at INRIA Sophia-Antipolis, and now heads the follow-up group 
                                GRAPHDECO. He is a INRIA Senior Researcher (full professor equivalent). He 
                                received the Eurographics (EG) Outstanding Technical Contributions award in 2007, 
                                the EG Distinguished Career Award in 2024 and is an EG fellow. He has received 
                                two prestigious ERC Advanced Grants in 2018 and in 2024. He was associate 
                                editor for ACM Trans. on Graphics, technical papers chair of SIGGRAPH Asia 
                                2010, co-chair of Eurographics IPC 2002 & 2008, chairs the ACM SIGGRAPH Papers 
                                Advisory Group and the EG working group on Rendering (EGSR). He has worked on 
                                many different topics in computer graphics, with an emphasis on rendering. 
                                He initially concentrated on lighting and shadow computation and subsequently 
                                worked on 3D audio, perceptually-driven algorithms, virtual reality and 3D 
                                interaction. He has worked on textures, weathering and perception for graphics 
                                and in recent years focused on novel-view synthesis, relighting as well as 
                                material acquisition often using deep learning methodologies. 
                            </p>
                            
                        </div>                 
                    </div>
                </div>
                <div class="text-sm px-6 text-justify pb-6 hidden">
                    <p><strong>Abstract:</strong> Neural rendering has advanced at outstanding speed in 
                        recent years, with the advent of Neural Radiance Fields (NeRFs), 
                        typically based on volumetric ray-marching. Last year, our group developed an 
                        alternative approach, 3D Gaussian Splatting, that has better performance for 
                        training, display speed and visual quality and has seen widespread adoption both 
                        academically and industrially. In this talk, we describe the 20+ year process 
                        leading to the development of this method and discuss some future directions. 
                        We will start with a short historical perspective of our work on image-based and 
                        neural rendering over the years, outlining several developments that guided our 
                        thinking over the years. We then discuss a sequence of three point-based 
                        rasterization methods for novel view synthesis -- developed in the context of G. 
                        Kopanas' Ph.D. and the ERC Advanced Grant FUNGRAPH -- that culminated with 3D 
                        Gaussian Splatting. We will emphasize how we progressively overcame the challenges as 
                        the research progressed. We first discuss differentiable point splatting and how 
                        we extended it in our first approach that enhances points with neural features, 
                        optimizing geometry to correct reconstruction errors. We briefly review our 
                        second method that handles highly reflective objects, where we use multi-layer 
                        perceptrons (MLP), to learn the motion of reflections and to perform the final 
                        rendering of captured scenes. We then discuss 3D Gaussian Splatting, that 
                        provides the high-quality real-time rendering for novel view synthesis using a 
                        novel 3D scene representation based on 3D Gaussians and fast GPU rasterization. 
                        We will conclude with a discussion of future directions for 3D Gaussian 
                        splatting with examples from recent work.
                    </p>
                    <p><strong>Bio:</strong> George Drettakis graduated in Computer Science from the 
                        University of Crete, Greece, and obtained an M.Sc. and a Ph.D., (1994) at the 
                        University of Toronto, with E. Fiume. After an ERCIM postdoc in Grenoble, 
                        Barcelona and Bonn, he obtained a Inria researcher position in Grenoble in 1995, 
                        and his "Habilitation" at the University of Grenoble (1999). He then founded the 
                        REVES research group at INRIA Sophia-Antipolis, and now heads the follow-up group 
                        GRAPHDECO. He is a INRIA Senior Researcher (full professor equivalent). He 
                        received the Eurographics (EG) Outstanding Technical Contributions award in 2007, 
                        the EG Distinguished Career Award in 2024 and is an EG fellow. He has received 
                        two prestigious ERC Advanced Grants in 2018 and in 2024. He was associate 
                        editor for ACM Trans. on Graphics, technical papers chair of SIGGRAPH Asia 
                        2010, co-chair of Eurographics IPC 2002 & 2008, chairs the ACM SIGGRAPH Papers 
                        Advisory Group and the EG working group on Rendering (EGSR). He has worked on 
                        many different topics in computer graphics, with an emphasis on rendering. 
                        He initially concentrated on lighting and shadow computation and subsequently 
                        worked on 3D audio, perceptually-driven algorithms, virtual reality and 3D 
                        interaction. He has worked on textures, weathering and perception for graphics 
                        and in recent years focused on novel-view synthesis, relighting as well as 
                        material acquisition often using deep learning methodologies.
                    </p>
                </div>
            </div>
            <!-- <div class="flex flex-col lg:flex-row items-center lg:items-start w-full p-2 lg:p-5">
                We will be adding details of couple more plenary speakers soon.
            </div>             -->
        </div>





        <!-- Footer -->
        <footer class="bg-navigationPurple-700">
            <div class="flex flex-col lg:flex-row text-white items-center lg:items-start lg:justify-between p-3 lg:p-10">
                <div class="w-1/3 order-1 lg:order-2 flex justify-center">
                    <img width="276px" src="assets/logo.svg" alt="icvgip-logo">
                </div>
                <div class="w-full lg:w-1/3 flex flex-col order-3 items-center lg:items-start text-center lg:text-left lg:order-1 px-16 pt-5 lg:px-0 lg:pt-0 text-sm">
                    <div class="pb-2">Institute Address</div>
                    <div class="w-2/3 h-[1px] bg-white"></div>
                    <div class="pt-2 lg:w-2/3 ">26/C, Opposite of Infosys gate 10, Electronics City Phase 1, Hosur Road, Bengaluru - 560100</div>
                </div>
                <div id="socials" class="flex flex-col w-1/3 order-2 items-center justify-center lg:items-end lg:order-3 pt-5 lg:pt-0">
                    <div class="hidden lg:block pb-2 text-sm">Socials</div>
                    <div class="w-2/3 lg:w-1/3 h-[1px] bg-white"></div>
                    <div class="flex flex-row w-full justify-center lg:justify-end gap-2 pt-5 lg:pt-2">
                        <a class="text-white hover:text-blue-400" aria-label="LinkedIn" href="" target="_blank">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="h-4">
                                <path fill="currentColor"
                                    d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z">
                                </path>
                            </svg>
                        </a>
                        <a class="text-white hover:text-black" aria-label="X" href="" target="_blank">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 30 30"  class="h-4">
                                <path fill="currentColor" 
                                    d="M26.37,26l-8.795-12.822l0.015,0.012L25.52,4h-2.65l-6.46,7.48L11.28,4H4.33l8.211,11.971L12.54,15.97L3.88,26h2.65 l7.182-8.322L19.42,26H26.37z M10.23,6l12.34,18h-2.1L8.12,6H10.23z">
                                </path>
                            </svg>
                        </a>
                    </div>
                    
                </div>
                
            </div>
            <div id="copyright" class="text-white text-center text-xs p-2">
                © 2024 IIIT-Bangalore. All Rights Reserved.
            </div>
        </footer>
        <!-- =============================================================== -->

    </div>

    <!-- <div class="bg-slate-400 container-sm mx-auto h-10">

    </div> -->
    <script src="index.js"></script>
    <!-- <script src="organisers.js"></script> -->

</body>

</html>